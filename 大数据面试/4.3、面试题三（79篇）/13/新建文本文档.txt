新华网

spark集群搭建步骤：
1、上传、解压部署包
2、配置slaves，来指定worker节点服务器
3、配置spark-env.sh，指定Master节点，指定任务提交的端口，指定每个worker所支配的core的个数，指定每个worker节点管理的内存
4、发送到其他节点
5、启动spark，./start-all.sh
6、WEBUI查看是否搭建成功，端口8080（可修改）





spark-submit 指定外部jar包的方式：./spark-submit --master spark://node01:7077 --class ... jar ...





Storm是最佳的流式计算框架，Storm由Java和Clojure写成，Storm的优点是全内存计算，所以它的定位是分布式实时计算系统，按照Storm作者的说法，Storm对于实时计算的意义类似于Hadoop对于批处理的意义。
Storm的适用场景：
1）流数据处理
Storm可以用来处理源源不断流进来的消息，处理之后将结果写入到某个存储中去。
2）分布式RPC。由于Storm的处理组件是分布式的，而且处理延迟极低，所以可以作为一个通用的分布式RPC框架来使用。

SparkSpark是一个基于内存计算的开源集群计算系统，目的是更快速的进行数据分析。Spark由加州伯克利大学AMP实验室Matei为主的小团队使用Scala开发开发，类似于Hadoop MapReduce的通用并行计算框架，Spark基于Map Reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的Map Reduce的算法。
Spark的适用场景：
1）多次操作特定数据集的应用场合
Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。
2）粗粒度更新状态的应用
由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如Web服务的存储或者是增量的Web爬虫和索引。就是对于那种增量修改的应用模型不适合。
总的来说Spark的适用面比较广泛且比较通用。

Hadoop是实现了MapReduce的思想，将数据切片计算来处理大量的离线数据数据。Hadoop处理的数据必须是已经存放在HDFS上或者类似HBase的数据库中，所以Hadoop实现的时候是通过移动计算到这些存放数据的机器上来提高效率。
Hadoop的适用场景：
1）海量数据的离线分析处理
2）大规模Web信息搜索
3）数据密集型并行计算

简单来说：
Hadoop适合于离线的批量数据处理适用于对实时性要求极低的场景
Storm适合于实时流数据处理，实时性方面做得极好
Spark是内存分布式计算框架，试图吞并Hadoop的Map-Reduce批处理框架和Storm的流处理框架，但是Spark已经做得很不错了，批处理方面性能优于Map-Reduce，但是流处理目前还是弱于Storm，产品仍在改进之中










3.HBase表中的每个列都归属于某个列族，列族必须作为表模式(schema)定义的一部分预先给出； 列名以列族作为前缀，每个“列族”都可以有多个列成员(column)； HBase把同一列族里面的数据存储在同一目录下，由几个文件保存。



15:BCD 
16:A
17:BD
18:A
19:D
20:B
14:B


