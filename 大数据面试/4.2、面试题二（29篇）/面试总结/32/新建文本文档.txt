相同点：都实现了Map接口，都是轻量级的实现。两者采用的Hash算法几乎一样，所以性能不会有很大的差异。
1.语法上面的区别：
1）HashMap允许键值为空，Hashtable不允许。
2）HashMap包含了containsvalue和containsKey，不包含有contains。
2.安全方面的区别
HashTable支持线程安全的，而HashMap不支持线程同步，是非线程安全的。因此，HashMap相对来说效率可能会高于Hashtable。
3.源码级别的区别
Hashtable，hash数组默认的大小是11，增加的方式是old*2+1,而HashMap中，hash数组的默认大小是16，而且一定是2的指数。
相较于HashMap和HashTable，TreeMap是利用红黑树来实现的，实现了SortMap接口，能够对保存的记录根据键进行排序。所以一般需要排序的情况下是选择TreeMap来进行。





1.4题
public class Test {
      public static void main(String[] args) {
        int[] arr = { 1,20,50,30,10,90,7,12,100,40,8};
        int searchWord = 20; // 所要查找的数
        Arrays.sort(arr); //二分法查找之前，一定要对数组元素排序
        System.out.println(Arrays.toString(arr));
        System.out.println(searchWord+"元素的索引："+b




朴素贝叶斯是假设时间A与B发生的概率是独立的，没有相互影响的，但现实生活中两个事件可能会有关系存在。因为利用假设来简化了问题，所以朴素


L1和L2的定义
L1是绝对值之和，L2是平方之和。
更深层的含义
L1追求的是稀疏，可以理解为变量个数少，L2主要用于处理过拟合问题，让每个权重参数值小？！
L2能加速训练？！





adaboost简介
adaboost算法是将一系列弱分类器组合成一个强分类器的算法。所谓弱分类器是指识别错误率小于1/2，即准确率仅比随机猜测略高的学习算法。强分类器是指识别准确率很高并能在多项式时间内完成的学习算法。1 
Adaboost算法依次训练弱分类器，赋予错误率低的分类器高权值，错误率高的分类器低权值，使得准确性高的分类器获得更高的‘话语权’，从而构造出一个强分类器。 
可以想到，Adaboost算法主要包括两个部分： 
1. 弱分类器的训练 
2. 分类器权值的计算


主要相同点：Lock能完成synchronized所实现的所有功能
主要不同点：Lock有比synchronized更精确的线程语义和更好的性能。
synchronized会自动释放锁，而Lock一定要求程序员手工释放，并且必须在finally从句中释放。




传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 
xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 
Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 
列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 
xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。






合理的设置并行度
理想情况下,task数量设置成Spark Application 的总CPU核数,但是现实中总有一些task运行慢一些task快,导致快的先执行完,空余的cpu 核就浪费掉了,所以官方推荐task数量要设置成Spark Application的总cpu核数的2~3 倍
如何设置并行度
SparkConf conf = new SparkConf()
  .set("spark.default.parallelism", "500")


  重构RDD架构及RDD持久化 序列化 
  
  
  
  Kryo序列化的使用
  
  
shuffle调优之合并map端输出文件
默认情况下,Spark是不开启合并map端输出文件机制的,所以当分批次执行task时,每批的task都会创建新的文件,而不会共用,大大影响了性能,所以当有大量map文件生成时,需要开启该机制 
设置方法
new SparkConf().set("spark.shuffle.consolidateFiles", "true")  
  
  
  
  
  
  